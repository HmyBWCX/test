{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HmyBWCX/test/blob/main/phi_3_mini_4k_instruct_unsloth_2x_faster_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + â­ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> â­\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth#installation-instructions---conda).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).\n",
        "\n",
        "We'll use the `Phi-3` format for conversational finetunes. We use the [Open Assistant dataset](https://huggingface.co/datasets/philschmid/guanaco-sharegpt-style) in ShareGPT style."
      ],
      "metadata": {
        "id": "IqM-T1RTzY6C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages! å®‰è£…åŒ…\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹\n",
        "load_in_4bit = True # ä½¿ç”¨å››ä½é‡åŒ–\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [   #è¿™æ˜¯ä¸€ä¸ªé¢„å…ˆé‡åŒ–çš„æ¨¡å‹åˆ—è¡¨\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-3-mini-4k-instruct\", # æˆ‘é€‰ç”¨çš„æ¨¡å‹åç§°\n",
        "    max_seq_length = max_seq_length,        # ä¿®æ”¹æœ€å¤§åºåˆ—é•¿åº¦\n",
        "    dtype = dtype,                 # è®¾ç½®æ•°æ®ç±»å‹ã€‚Noneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ã€‚ä½ å¯ä»¥æ ¹æ®éœ€è¦å°†ä½ çš„GPUè®¾ç½®ä¸ºtorch.float16æˆ–torch.bfloat16ã€‚\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ],
      "metadata": {
        "id": "SXd9bTZd1aaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # å¤§çŸ©é˜µæŒ‰ç…§ä¸­é—´å€¼åˆ’åˆ†ä¸ºå°çŸ©é˜µ\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,    # lora_alphaæ˜¯LoRAçš„ç¼©æ”¾å› å­ï¼Œç”¨äºè°ƒæ•´ä½ç§©çŸ©é˜µçš„å½±å“åŠ›ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒæ§åˆ¶äº†LoRAå¯¹æ¨¡å‹å‚æ•°çš„è°ƒæ•´åŠ›åº¦ã€‚\n",
        "    lora_dropout = 0,    # lora_dropoutæ˜¯LoRAçš„dropoutç‡ï¼Œè®¾ç½®ä¸º0è¡¨ç¤ºä¸ä½¿ç”¨dropoutã€‚dropoutæ˜¯ä¸€ç§é˜²æ­¢è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ï¼Œä½†åœ¨è¿™é‡Œè®¾ç½®ä¸º0å¯ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚\n",
        "    bias = \"none\",    # biasæ˜¯LoRAçš„åç½®é¡¹è®¾ç½®ï¼Œè®¾ç½®ä¸º\"none\"è¡¨ç¤ºä¸ä½¿ç”¨åç½®ã€‚åç½®é¡¹æ˜¯ç¥ç»ç½‘ç»œä¸­çš„ä¸€ä¸ªå‚æ•°ï¼Œä½†åœ¨è¿™é‡Œä¸ä½¿ç”¨å®ƒå¯ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚\n",
        "\n",
        "    use_gradient_checkpointing = \"unsloth\", # use_gradient_checkpointingä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯æ¥å‡å°‘æ˜¾å­˜ä½¿ç”¨ã€‚\"unsloth\"è¡¨ç¤ºä½¿ç”¨unslothåº“çš„ä¼˜åŒ–ç‰ˆæœ¬ã€‚æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯æ˜¯ä¸€ç§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜ä¸­é—´è®¡ç®—ç»“æœçš„æ–¹æ³•ï¼Œä»è€Œå‡å°‘æ˜¾å­˜çš„ä½¿ç”¨ã€‚\n",
        "    random_state = 3407,      # random_stateè®¾ç½®äº†éšæœºç§å­ï¼Œä»¥ç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§ã€‚éšæœºç§å­æ˜¯ä¸€ç§åˆå§‹åŒ–éšæœºæ•°ç”Ÿæˆå™¨çš„æ–¹å¼ï¼Œä½¿å¾—æ¯æ¬¡è¿è¡Œä»£ç æ—¶ï¼Œå¾—åˆ°çš„ç»“æœéƒ½æ˜¯ä¸€è‡´çš„ã€‚\n",
        "    use_rslora = False,  # use_rsloraè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨ç§©ç¨³å®šåŒ–LoRAï¼ˆRank Stabilized LoRAï¼‰ã€‚è®¾ç½®ä¸ºFalseè¡¨ç¤ºä¸ä½¿ç”¨ã€‚ç§©ç¨³å®šåŒ–LoRAæ˜¯ä¸€ç§æ”¹è¿›çš„LoRAæŠ€æœ¯ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„ç¨³å®šæ€§ã€‚\n",
        "    loftq_config = None, # loftq_configè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨LoftQé…ç½®ã€‚è®¾ç½®ä¸ºNoneè¡¨ç¤ºä¸ä½¿ç”¨ã€‚LoftQæ˜¯ä¸€ç§é‡åŒ–æŠ€æœ¯ï¼Œå¯ä»¥è¿›ä¸€æ­¥å‡å°‘æ¨¡å‹çš„è®¡ç®—é‡å’Œå†…å­˜ä½¿ç”¨ã€‚\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template #è·å–èŠå¤©æ¨¡æ¿\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-3\", # æŒ‡å®šèŠå¤©æ¨¡æ¿çš„ç±»å‹ å¯ä»¥æ¢æˆllama\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # äººè¯è½¬ä¸ºæœºå™¨çš„è¯ eg: content(å…·ä½“æ¶ˆæ¯æ–‡æœ¬) valueè¿™æ˜¯æ¨¡å‹æœŸæœ›çš„å­—æ®µåï¼Œè¡¨ç¤ºæ¶ˆæ¯çš„å†…å®¹\n",
        "    # ä½¿ç”¨mappingï¼Œå°†åŸå§‹æ•°æ®æ ¼å¼è½¬æ¢æˆæ¨¡å‹æœŸæœ›çš„æ ¼å¼ï¼Œä»è€Œç¡®ä¿æ•°æ®èƒ½å¤Ÿè¢«æ­£ç¡®å¤„ç†\n",
        "    # \"role\": \"from\"ï¼šå°†åŸå§‹æ•°æ®ä¸­çš„è§’è‰²å­—æ®µæ˜ å°„ä¸ºæ¨¡å‹æœŸæœ›çš„å‘é€è€…å­—æ®µã€‚\n",
        "    # \"content\": \"value\"ï¼šå°†åŸå§‹æ•°æ®ä¸­çš„å†…å®¹å­—æ®µæ˜ å°„ä¸ºæ¨¡å‹æœŸæœ›çš„æ¶ˆæ¯å†…å®¹å­—æ®µã€‚\n",
        "    # \"user\": \"human\"ï¼šå°†åŸå§‹æ•°æ®ä¸­çš„ç”¨æˆ·è§’è‰²æ˜ å°„ä¸ºæ¨¡å‹æœŸæœ›çš„äººç±»ç”¨æˆ·è§’è‰²ã€‚\n",
        "    # \"assistant\": \"gpt\"ï¼šå°†åŸå§‹æ•°æ®ä¸­çš„åŠ©æ‰‹è§’è‰²æ˜ å°„ä¸ºæ¨¡å‹æœŸæœ›çš„GPTæ¨¡å‹è§’è‰²ã€‚\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):  # è¿™ä¸ªå‡½æ•°æ¥å—ä¸€ä¸ªå‚æ•°examplesï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªåŒ…å«å¯¹è¯æ•°æ®çš„å­—å…¸æˆ–åˆ—è¡¨ã€‚\n",
        "    convos = examples[\"conversations\"]  #ä»examplesä¸­æå–å¯¹è¯convosã€‚\n",
        "\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    #åˆ†æ­¥éª¤ç†è§£\n",
        "    #texts = [ ... for convo in convos] è¿™æ˜¯ä¸€ç§åˆ—è¡¨æ¨å¯¼å¼ï¼Œç”¨äºç”Ÿæˆä¸€ä¸ªæ–°çš„åˆ—è¡¨ã€‚å®ƒä¼šéå†convosä¸­çš„æ¯ä¸ªå¯¹è¯convoï¼Œå¹¶å¯¹æ¯ä¸ªå¯¹è¯åº”ç”¨tokenizer.apply_chat_templateå‡½æ•°ï¼Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ªåŒ…å«æ‰€æœ‰æ ¼å¼åŒ–æ–‡æœ¬çš„åˆ—è¡¨textsã€‚\n",
        "    return { \"text\" : texts, } #è¿”å›ä¸€ä¸ªåŒ…å«æ ¼å¼åŒ–æ–‡æœ¬çš„å­—å…¸\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"philschmid/guanaco-sharegpt-style\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "Edrn7Rxmojtu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how the `Phi-3` format works by printing the 5th element"
      ],
      "metadata": {
        "id": "cHiVoToneynS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[5][\"conversations\"]"
      ],
      "metadata": {
        "id": "4GSuKSSbpYKq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[5][\"text\"])"
      ],
      "metadata": {
        "id": "U5iEWrUkevpE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " æ•°æ®         "
      ],
      "metadata": {
        "id": "GuKOAUDpUeDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unsloth_template = \"\"\"\n",
        "    {{ bos_token }} #å¼€å§‹æ ‡è®°\n",
        "    Hello, æˆ‘æ˜¯ä½ çš„ä¸“å±å°åŠ©æ‰‹ï¼Œä½ æœ‰ä»»ä½•é—®é¢˜éƒ½å¯ä»¥é—®æˆ‘å“¦ï¼Œä¸ºä½ ä¸Šä¹å¤©æ½æœˆï¼Œä¸‹äº”æ´‹æ‰é³–ï¼\n",
        "    {% for message in messages %}\n",
        "        {% if message['role'] == 'user' %}\n",
        "            >>> Userè¯´: {{ message['content'] }}\n",
        "        {% elif message['role'] == 'assistant' %}\n",
        "            >>> å°åŠ©æ‰‹è¯´: {{ message['content'] }} {{ eos_token }}\n",
        "        {% endif %}\n",
        "    {% endfor %}\n",
        "    {% if add_generation_prompt %}\n",
        "        >>> æˆ‘æ­£åœ¨é£å¾€ä¹å¤©ï¼Œæ½œå…¥äº”æ´‹ï¼Œè¯·ä½ ç­‰æˆ‘ä¸€ä¼šå“¦.....\n",
        "    {% endif %}\n",
        "\"\"\"\n",
        "unsloth_eos_token = \"bye\"\n",
        "\n",
        "if False:\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template = (unsloth_template, unsloth_eos_token,), # ä½ å¿…é¡»æä¾›ä¸€ä¸ªæ¨¡æ¿å’Œç»“æŸæ ‡è®°\n",
        "        mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPTé£æ ¼\n",
        "        map_eos_token = True, # å°†ç»“æŸæ ‡è®°æ˜ å°„ä¸º</s>\n",
        "\n",
        "        )"
      ],
      "metadata": {
        "id": "p31Z-S6FUieB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ],
      "metadata": {
        "id": "idAEIeSQ3xdS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,   #æ¯æ¬¡è®­ç»ƒæ—¶ï¼Œæ¨¡å‹å¤„ç†çš„æ•°æ®é‡ã€‚æ‰¹æ¬¡è¶Šå¤§ï¼Œè®­ç»ƒè¶Šå¿«ï¼Œä½†éœ€è¦æ›´å¤šçš„å†…å­˜ã€‚            å†…å­˜ä¸è¶³å‡å°ï¼Œå……è¶³å¢å¤§\n",
        "        gradient_accumulation_steps = 4,   #å°†å¤šä¸ªå°æ‰¹æ¬¡çš„æ¢¯åº¦ç´¯ç§¯èµ·æ¥ï¼Œæ¨¡æ‹Ÿä¸€ä¸ªå¤§æ‰¹æ¬¡çš„è®­ç»ƒæ•ˆæœã€‚(ä»–è¦å’Œç¬¬ä¸€æ­¥ç›¸ä¹˜æ‹¿åˆ°æ¯æ¬¡æ›´æ–°å¤šå°‘ä¸ªæ ·æœ¬)\n",
        "        warmup_steps = 5,          #åœ¨è®­ç»ƒå¼€å§‹æ—¶é€æ­¥å¢åŠ å­¦ä¹ ç‡ï¼Œä»¥é¿å…æ¨¡å‹å‚æ•°å‰§çƒˆå˜åŒ–ã€‚                     è®­ç»ƒåˆæœŸä¸ç¨³å®šå¢åŠ ï¼Œç¨³å®šåˆ™å‡å°\n",
        "        max_steps = 60,           #è®­ç»ƒçš„æ€»æ­¥æ•°ï¼Œå†³å®šè®­ç»ƒçš„æ€»æ—¶é—´ã€‚                               å¿«é€Ÿæµ‹è¯•å‡å°ï¼Œå……åˆ†è®­ç»ƒå¢å¤§\n",
        "        learning_rate = 2e-4,        #æ§åˆ¶æ¨¡å‹å‚æ•°æ›´æ–°çš„æ­¥ä¼ï¼Œå­¦ä¹ ç‡è¶Šå¤§ï¼Œå‚æ•°æ›´æ–°è¶Šå¿«ã€‚                      ä¸ç¨³å®šå‡å°ï¼Œé€Ÿåº¦æ…¢å¢å¤§\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,          #æ§åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­è®°å½•æ—¥å¿—çš„é¢‘ç‡ã€‚                               å¸Œæœ›æ›´é¢‘ç¹åœ°æŸ¥çœ‹è®­ç»ƒè¿›å±•å‡å°ï¼Œå¸Œæœ›å‡å°‘æ—¥å¿—è®°å½•å¯¹è®­ç»ƒçš„å½±å“å¢å¤§\n",
        "        optim = \"adamw_8bit\",         #é€‰æ‹©ç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°çš„ä¼˜åŒ–ç®—æ³•ã€‚                              adamw_8bitèŠ‚çœå†…å­˜\n",
        "        weight_decay = 0.01,          #é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œé€šè¿‡åœ¨æ›´æ–°å‚æ•°æ—¶æ–½åŠ æƒ©ç½šã€‚ 0.01 åˆ° 0.1\n",
        "        #æ¨¡å‹è¿‡æ‹Ÿåˆï¼šæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æ–°æ•°æ®ä¸Šè¡¨ç°ä¸å¥½ã€‚è§£å†³æ–¹æ³•åŒ…æ‹¬å¢åŠ æƒé‡è¡°å‡ã€å‡å°‘æ¨¡å‹å¤æ‚åº¦ã€å¢åŠ è®­ç»ƒæ•°æ®å’Œä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯ã€‚\n",
        "#        #æ¨¡å‹æ¬ æ‹Ÿåˆï¼šæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®å’Œæ–°æ•°æ®ä¸Šéƒ½è¡¨ç°ä¸å¥½ã€‚è§£å†³æ–¹æ³•åŒ…æ‹¬å‡å°‘æƒé‡è¡°å‡ã€å¢åŠ æ¨¡å‹å¤æ‚åº¦ã€å¢åŠ è®­ç»ƒæ—¶é—´å’Œè°ƒæ•´å­¦ä¹ ç‡ã€‚\n",
        "        lr_scheduler_type = \"linear\",     #æ§åˆ¶å­¦ä¹ ç‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–æ–¹å¼ã€‚\n",
        "        seed = 3407,              #ç¡®ä¿æ¯æ¬¡è®­ç»ƒçš„ç»“æœä¸€è‡´ï¼Œä¾¿äºå¤ç°å®éªŒç»“æœ 42æˆ–è€…3407                   å¯å¤ç°42æˆ–è€…3407ï¼Œä¸éœ€è¦ç»“æœå¯å¤ç°å°±ä¸è®¾ç½®\n",
        "        output_dir = \"outputs\",         #ä¿å­˜è®­ç»ƒç»“æœå’Œæ¨¡å‹çš„ç›®å½•ã€‚\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "# Check if a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    gpu_stats = torch.cuda.get_device_properties(0)\n",
        "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "    print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "    print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "else:\n",
        "    print(\"No GPU available, using CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqnaKmlO1U9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! Since we're using `Phi-3`, use `apply_chat_template` with `add_generation_prompt` set to `True` for inference."
      ],
      "metadata": {
        "id": "ekOmTR1hSNcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ],
      "metadata": {
        "id": "CrSvZObor0lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
      ],
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ],
      "metadata": {
        "id": "uMuVrWbjAzhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ],
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ],
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"What is a famous tall tower in Paris?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
      ],
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ],
      "metadata": {
        "id": "QQMjaNrjsU5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoModelForPeftCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoModelForPeftCausalLM.from_pretrained(\n",
        "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ],
      "metadata": {
        "id": "f422JgM9sdVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ],
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
      ],
      "metadata": {
        "id": "TCv4vXHd61i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
      ],
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html)."
      ],
      "metadata": {
        "id": "bDp0zNpwe6U_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
        "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ğŸ¤— HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
        "7. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
        "9. Gemma 6 trillion tokens is 2.5x faster! [free Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
        "</div>"
      ],
      "metadata": {
        "id": "Zt9CHJqO6p30"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}